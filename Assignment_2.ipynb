{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e7a7c15-cdb1-4595-a22b-56806e51c1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import downloader\n",
    "from gensim.models import KeyedVectors\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c389e43b-f466-4992-8d76-d1719f79ecd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_details(dataframe,wv_model):\n",
    "\n",
    "    details_list = []\n",
    "    #going through dataset and evaluating the closest synonym to the question\n",
    "    for index, row in dataframe.iterrows():\n",
    "        question=row['question']\n",
    "        answer=row['answer']\n",
    "        system_guess_word=''\n",
    "        best_choice_similarity=-1\n",
    "        \n",
    "        if question in wv_model.key_to_index:        \n",
    "            for index in range(0,4):\n",
    "                choice=row[str(index)]\n",
    "                if choice in wv_model.key_to_index:\n",
    "                    choice_similarity=wv_model.similarity(question,choice)\n",
    "                    if choice_similarity>best_choice_similarity:\n",
    "                        best_choice_similarity=choice_similarity\n",
    "                        system_guess_word=choice\n",
    "        \n",
    "        #validate label\n",
    "        if best_choice_similarity==-1:\n",
    "            label='guess'\n",
    "            #If label is \"guess\" then choose the system word randomly\n",
    "            random_column = str(random.randint(0,3))\n",
    "            system_guess_word= row[random_column]\n",
    "        elif answer==system_guess_word:\n",
    "            label='correct'\n",
    "        else:\n",
    "            label='wrong'\n",
    "        #add each row to the list\n",
    "        details_list.append([question,answer,system_guess_word,label])\n",
    "        #print(f\"{question},{answer},{system_guess_word},{label}\")\n",
    "    \n",
    "    return details_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da6f3645-2bba-4d88-94c7-7f0b7b91e424",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate output file with details\n",
    "def generate_details_file(model_name,details_list):\n",
    "    output_file_name=f\"{model_name}-details.csv\"\n",
    "    with open(output_file_name, 'w') as output_file:\n",
    "        for row in details_list:            \n",
    "            for index in range(0,4):\n",
    "                if(index==0):                    \n",
    "                    output_file.write(row[index])  \n",
    "                else:\n",
    "                    output_file.write(',' + row[index])\n",
    "            output_file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89f3a091-84bc-4ec2-9414-a895af549cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#append results from each model to analysis file\n",
    "def append_results(model_name,vocabulary_size,details_list):\n",
    "    \n",
    "    output_file_name = \"analysis.csv\"\n",
    "    #C is count_correct\n",
    "    C = 0\n",
    "    count_guess = 0\n",
    "    \n",
    "    for row in details_list:\n",
    "        if row[3] == 'correct':            \n",
    "            C +=1\n",
    "        elif row[3] == 'guess':\n",
    "            count_guess +=1\n",
    "    #V  is number of questions without guess\n",
    "    V = len(details_list) - count_guess\n",
    "    model_accuracy = C / V\n",
    "\n",
    "    with open(output_file_name, 'a') as output_file:\n",
    "         output_file.write(f\"{model_name},{vocabulary_size},{C},{V},{model_accuracy}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00292344-1062-4f63-9d28-92688da43654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataframe = pd.read_csv('synonym.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73ee69a2-5dc3-487d-8fcd-5bbcceba384f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1078677-225b-452e-920e-b3b9ec40c818",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load wv_google\n",
    "wv_google=downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8628b3d2-b3b8-47f9-bf95-63a7f4eb7b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_details = get_details(dataframe,wv_google)\n",
    "generate_details_file('word2vec-google-news-300',list_details)\n",
    "vocabulary_size = len(wv_google.key_to_index)\n",
    "append_results('word2vec-google-news-300',vocabulary_size,list_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4e8a070-dc66-4318-9c3d-906ea24a10ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"#Task 2\\n# 2 new models from different corpora, but with the same embedding size 300\\n\\nc1_e1 = 'fasttext-wiki-news-subwords-300'\\nmodel = downloader.load(c1_e1)\\nlist_details = get_details(dataframe,model)\\ngenerate_details_file(c1_e1,list_details)\\nvocabulary_size = len(model.key_to_index)\\nappend_results(c1_e1,vocabulary_size,list_details)\\n\\nc2_e2 = 'glove-wiki-gigaword-300'\\nmodel = downloader.load(c2_e2)\\nlist_details = get_details(dataframe,model)\\ngenerate_details_file(c2_e2,list_details)\\nvocabulary_size = len(model.key_to_index)\\nappend_results(c2_e2,vocabulary_size,list_details)\\n\\n# 2 models from the same corpus but different embedding sizes\\nc3_e3 ='glove-twitter-100'\\nmodel = downloader.load(c3_e3)\\nlist_details = get_details(dataframe,model)\\ngenerate_details_file(c3_e3,list_details)\\nvocabulary_size = len(model.key_to_index)\\nappend_results(c3_e3,vocabulary_size,list_details)\\n\\n\\nc4_e4 ='glove-twitter-200'\\nmodel = downloader.load(c4_e4)\\nlist_details = get_details(dataframe,model)\\ngenerate_details_file(c4_e4,list_details)\\nvocabulary_size = len(model.key_to_index)\\nappend_results(c4_e4,vocabulary_size,list_details)\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "c1_e1 = 'fasttext-wiki-news-subwords-300'\n",
    "model = downloader.load(c1_e1)\n",
    "list_details = get_details(dataframe,model)\n",
    "generate_details_file(c1_e1,list_details)\n",
    "vocabulary_size = len(model.key_to_index)\n",
    "append_results(c1_e1,vocabulary_size,list_details)\n",
    "\n",
    "c2_e2 = 'glove-wiki-gigaword-300'\n",
    "model = downloader.load(c2_e2)\n",
    "list_details = get_details(dataframe,model)\n",
    "generate_details_file(c2_e2,list_details)\n",
    "vocabulary_size = len(model.key_to_index)\n",
    "append_results(c2_e2,vocabulary_size,list_details)\n",
    "\n",
    "# 2 models from the same corpus but different embedding sizes\n",
    "c3_e3 ='glove-twitter-100'\n",
    "model = downloader.load(c3_e3)\n",
    "list_details = get_details(dataframe,model)\n",
    "generate_details_file(c3_e3,list_details)\n",
    "vocabulary_size = len(model.key_to_index)\n",
    "append_results(c3_e3,vocabulary_size,list_details)\n",
    "\n",
    "\n",
    "c4_e4 ='glove-twitter-200'\n",
    "model = downloader.load(c4_e4)\n",
    "list_details = get_details(dataframe,model)\n",
    "generate_details_file(c4_e4,list_details)\n",
    "vocabulary_size = len(model.key_to_index)\n",
    "append_results(c4_e4,vocabulary_size,list_details)'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "923fe4db-5a48-45ed-b928-948933eaf985",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 2\n",
    "# 2 new models from different corpora, but with the same embedding size 100\n",
    "\n",
    "c1_e1 = 'English-CoNLL17-100'\n",
    "model = KeyedVectors.load_word2vec_format('./40/model.bin', binary=True)\n",
    "list_details = get_details(dataframe,model)\n",
    "generate_details_file(c1_e1,list_details)\n",
    "vocabulary_size = len(model.key_to_index)\n",
    "append_results(c1_e1,vocabulary_size,list_details)\n",
    "\n",
    "c2_e2 = 'glove-wiki-gigaword-100'\n",
    "model = downloader.load(c2_e2)\n",
    "list_details = get_details(dataframe,model)\n",
    "generate_details_file(c2_e2,list_details)\n",
    "vocabulary_size = len(model.key_to_index)\n",
    "append_results(c2_e2,vocabulary_size,list_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e172e194-4a4a-46ba-8db3-1829280fcd39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
     ]
    }
   ],
   "source": [
    "# 2 models from the same corpus but different embedding sizes\n",
    "c3_e3 ='glove-twitter-100'\n",
    "model = downloader.load(c3_e3)\n",
    "list_details = get_details(dataframe,model)\n",
    "generate_details_file(c3_e3,list_details)\n",
    "vocabulary_size = len(model.key_to_index)\n",
    "append_results(c3_e3,vocabulary_size,list_details)\n",
    "\n",
    "c4_e4 ='glove-twitter-200'\n",
    "model = downloader.load(c4_e4)\n",
    "list_details = get_details(dataframe,model)\n",
    "generate_details_file(c4_e4,list_details)\n",
    "vocabulary_size = len(model.key_to_index)\n",
    "append_results(c4_e4,vocabulary_size,list_details)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
